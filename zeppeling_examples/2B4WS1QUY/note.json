{
  "paragraphs": [
    {
      "text": "%pyspark\nprint sc.version",
      "dateUpdated": "Dec 15, 2015 2:30:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446289941768_-374219316",
      "id": "20151031-111221_9258217",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "1.5.2\n"
      },
      "dateCreated": "Oct 31, 2015 11:12:21 AM",
      "dateStarted": "Dec 15, 2015 2:30:14 PM",
      "dateFinished": "Dec 15, 2015 2:30:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nprint  sc._conf.getAll()",
      "dateUpdated": "Dec 15, 2015 2:30:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446151715916_-398519589",
      "id": "20151029-204835_15568985",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[(u\u0027spark.driver.memory\u0027, u\u00272G\u0027), (u\u0027spark.jars\u0027, u\u0027file:/opt/zeppelin/interpreter/spark/zeppelin-spark-0.6.0-incubating-SNAPSHOT.jar\u0027), (u\u0027spark.fileserver.uri\u0027, u\u0027http://10.20.30.100:47503\u0027), (u\u0027zeppelin.spark.concurrentSQL\u0027, u\u0027false\u0027), (u\u0027spark.driver.extraClassPath\u0027, u\u0027::/opt/zeppelin/interpreter/spark/zeppelin-spark-0.6.0-incubating-SNAPSHOT.jar\u0027), (u\u0027zeppelin.dep.localrepo\u0027, u\u0027local-repo\u0027), (u\u0027spark.localProperties.clone\u0027, u\u0027true\u0027), (u\u0027spark.serializer.objectStreamReset\u0027, u\u0027100\u0027), (u\u0027spark.master\u0027, u\u0027local[*]\u0027), (u\u0027spark.submit.deployMode\u0027, u\u0027client\u0027), (u\u0027spark.yarn.dist.files\u0027, u\u0027file:/opt/spark/python/lib/pyspark.zip,file:/opt/spark/python/lib/py4j-0.8.2.1-src.zip\u0027), (u\u0027zeppelin.spark.useHiveContext\u0027, u\u0027true\u0027), (u\u0027spark.submit.pyArchives\u0027, u\u0027pyspark.zip:py4j-0.8.2.1-src.zip\u0027), (u\u0027spark.executor.memory\u0027, u\u0027512m\u0027), (u\u0027args\u0027, u\u0027\u0027), (u\u0027master\u0027, u\u0027local[*]\u0027), (u\u0027spark.externalBlockStore.folderName\u0027, u\u0027spark-1b65aaa7-8b30-4f9f-b549-3ddc0ffb1c54\u0027), (u\u0027spark.driver.port\u0027, u\u002735277\u0027), (u\u0027zeppelin.pyspark.python\u0027, u\u0027python\u0027), (u\u0027spark.scheduler.mode\u0027, u\u0027FAIR\u0027), (u\u0027zeppelin.spark.maxResult\u0027, u\u00271000\u0027), (u\u0027spark.executor.id\u0027, u\u0027driver\u0027), (u\u0027spark.driver.host\u0027, u\u002710.20.30.100\u0027), (u\u0027spark.driver.extraJavaOptions\u0027, u\u0027 -Dspark.home\u003d/opt/spark -Dfile.encoding\u003dUTF-8 -Dspark.home\u003d/opt/spark -Dfile.encoding\u003dUTF-8 -Dzeppelin.log.file\u003d/tmp/zeppelin-logs/zeppelin-interpreter-spark-root-spark-master.log\u0027), (u\u0027spark.rdd.compress\u0027, u\u0027True\u0027), (u\u0027spark.executor.instances\u0027, u\u00271\u0027), (u\u0027spark.app.name\u0027, u\u0027Zeppelin\u0027), (u\u0027zeppelin.dep.additionalRemoteRepository\u0027, u\u0027spark-packages,http://dl.bintray.com/spark-packages/maven,false;\u0027), (u\u0027spark.home\u0027, u\u0027/opt/spark\u0027), (u\u0027spark.repl.class.uri\u0027, u\u0027http://10.20.30.100:55886\u0027), (u\u0027spark.app.id\u0027, u\u0027local-1450187141020\u0027)]\n"
      },
      "dateCreated": "Oct 29, 2015 8:48:35 PM",
      "dateStarted": "Dec 15, 2015 2:30:15 PM",
      "dateFinished": "Dec 15, 2015 2:30:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nrdd \u003d sc.parallelize([1, 5, 3, 4, 7, 2, 9, 7])\nprint rdd.takeOrdered(10, key\u003d(lambda x: -x))\nprint rdd.top(10)",
      "dateUpdated": "Dec 15, 2015 2:30:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446151662499_118906385",
      "id": "20151029-204742_12383234",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[9, 7, 7, 5, 4, 3, 2, 1]\n[9, 7, 7, 5, 4, 3, 2, 1]\n"
      },
      "dateCreated": "Oct 29, 2015 8:47:42 PM",
      "dateStarted": "Dec 15, 2015 2:30:22 PM",
      "dateFinished": "Dec 15, 2015 2:30:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rdd \u003d sc.parallelize(Array(1, 5, 3, 4, 7, 2, 9, 7))\nrdd.takeOrdered(10)\nrdd.top(10)",
      "dateUpdated": "Dec 15, 2015 2:30:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446152610338_1436220913",
      "id": "20151029-210330_26820737",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rdd: org.apache.spark.rdd.RDD[Int] \u003d ParallelCollectionRDD[72] at parallelize at \u003cconsole\u003e:26\nres23: Array[Int] \u003d Array(1, 2, 3, 4, 5, 7, 7, 9)\nres24: Array[Int] \u003d Array(9, 7, 7, 5, 4, 3, 2, 1)\n"
      },
      "dateCreated": "Oct 29, 2015 9:03:30 PM",
      "dateStarted": "Dec 15, 2015 2:30:25 PM",
      "dateFinished": "Dec 15, 2015 2:30:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nbankPython.printSchema",
      "dateUpdated": "Dec 15, 2015 2:53:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446406007233_1376322500",
      "id": "20151101-192647_7203514",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 178, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027bankScala\u0027 is not defined\n"
      },
      "dateCreated": "Nov 1, 2015 7:26:47 PM",
      "dateStarted": "Dec 15, 2015 2:52:47 PM",
      "dateFinished": "Dec 15, 2015 2:52:47 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nbankScala.registerTempTable(\"bankPython\")",
      "dateUpdated": "Dec 15, 2015 2:53:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450191167602_597481009",
      "id": "20151215-145247_13588633",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 178, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027bankScala\u0027 is not defined\n"
      },
      "dateCreated": "Dec 15, 2015 2:52:47 PM",
      "dateStarted": "Dec 15, 2015 2:52:58 PM",
      "dateFinished": "Dec 15, 2015 2:52:58 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect age, count(1) value\nfrom bankScala\nwhere age \u003c 30 \ngroup by age \norder by age",
      "dateUpdated": "Dec 15, 2015 2:53:37 PM",
      "config": {
        "colWidth": 4.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450191178860_2123025722",
      "id": "20151215-145258_9793686",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: no such table bankScala; line 2 pos 5\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:260)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:264)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:916)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:916)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:914)\n\tat org.apache.spark.sql.DataFrame.\u003cinit\u003e(DataFrame.scala:132)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:725)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:141)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Dec 15, 2015 2:52:58 PM",
      "dateStarted": "Dec 15, 2015 2:53:37 PM",
      "dateFinished": "Dec 15, 2015 2:53:39 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect age, count(1) value \nfrom bankScala \nwhere age \u003c ${maxAge\u003d45} \ngroup by age \norder by age",
      "dateUpdated": "Dec 15, 2015 2:54:33 PM",
      "config": {
        "colWidth": 4.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450191217873_-1467405041",
      "id": "20151215-145337_32108095",
      "dateCreated": "Dec 15, 2015 2:53:37 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql \nselect age, count(1) value \nfrom bankScala\nwhere marital\u003d\"${marital\u003dsingle,single|divorced|married}\"\ngroup by age \norder by age",
      "dateUpdated": "Dec 15, 2015 2:54:34 PM",
      "config": {
        "colWidth": 4.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450191238233_1232609498",
      "id": "20151215-145358_32561493",
      "dateCreated": "Dec 15, 2015 2:53:58 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Test pyspark",
  "id": "2B4WS1QUY",
  "angularObjects": {
    "2B5S894X6": [],
    "2B6Z5JQKS": [],
    "2B7MJ53H9": [],
    "2B967XE6T": [],
    "2B8ASWNGR": [],
    "2B8TUZJTC": [],
    "2B7G42EYZ": [],
    "2B63NRM4J": [],
    "2B6UEZNWS": [],
    "2B6NHWG9Y": [],
    "2B5U8KY8D": [],
    "2B8TKHK8A": [],
    "2B9GUPF5P": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}