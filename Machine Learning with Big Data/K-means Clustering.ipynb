{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering\n",
    "## Introduction  \n",
    "The following Python code demonstrates some basic Clustering techniques using the __k-means__ algorithm on Spark. In this example, we will use the __MLLib__ package: __Kmeans__. This algorithm does the following: \n",
    "1. Assign points to the \"closest\" cluster mean. \n",
    "2. Update the cluster mean.\n",
    "3. Iterate until the ssignments converge.  \n",
    "\n",
    "The main algorithm __cost__ involves calculating the distance between points and the current cluster centers (centroids). To test the algorithm, we will use the generate artifical data using another __MLLib__ module called `RandomRDD`, which generates random RDD's. One of the functions of `RandomRDD` is `normalVectorRDD`, which creates a matrix of random numbers from a standard normal distribution. \n",
    "\n",
    "__Note:__ The __cost__ funtion gets the __Sum of Squared Error__ and is used to compute the different __k-means__ runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 48, mean: [ 3.22923238  3.81929245], stdev: [ 2.05035446  2.52269532], max: [ 6.95641707  8.46783831], min: [-1.08338413 -0.49359456])\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "# To generate random data RDD we need need `RandomRDD`\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "# Generate random class data and add in a cluster center to random 2D points\n",
    "c1_v = RandomRDDs.normalVectorRDD(sc,20,2,numPartitions=2,seed=1L).map(lambda v:np.add([1,5],v))\n",
    "c2_v = RandomRDDs.normalVectorRDD(sc,16,2,numPartitions=2,seed=2L).map(lambda v:np.add([5,1],v))\n",
    "c3_v = RandomRDDs.normalVectorRDD(sc,12,2,numPartitions=2,seed=3L).map(lambda v:np.add([4,6],v))\n",
    "\n",
    "# Concatenate 2 RDDs with the `.union(other)` function\n",
    "tmp_c = c1_v.union(c2_v)\n",
    "my_data = tmp_c.union(c3_v)   #this now has all points, as RDD\n",
    "\n",
    "# View the sumamry statistics on the data\n",
    "print my_data.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we use __k-means++__, implemented as `k-means||` in __MLLib__, where: \n",
    "- __k__ is the number of desired clusters. \n",
    "- __maxIterations__ is the maximum number of iterations to run.\n",
    "- __initializationMode__ specifies either random initialization or initialization via `k-means||`.\n",
    "- __runs__ is the number of times to run the k-means algorithm. Since __k-means__ is not guaranteed to find a globally optimal solution, when run multiple times on a given dataset, the algorithm returns the best clustering result. \n",
    "- __initializationSteps__ determines the number of steps in the `k-means||` algorithm. \n",
    "- __epsilon__ determines the distance threshold within which we consider k-means to have converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'centers',\n",
       " 'clusterCenters',\n",
       " 'computeCost',\n",
       " 'k',\n",
       " 'load',\n",
       " 'predict',\n",
       " 'save']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using k-means++\n",
    "my_kmmodel = KMeans.train(my_data,k=1,\n",
    "               maxIterations=20,runs=1,\n",
    "               initializationMode='k-means||',seed=10L)\n",
    "\n",
    "# View the available functions avilable on the model\n",
    "dir(my_kmmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error using computeCost():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "507.26136309159807"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the `computeCost` to compute the Sum Squared Error\n",
    "print \"Sum of Squared Error using computeCost():\"\n",
    "my_kmmodel.computeCost(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ If the `computeCost` function is not available, the following code manualy does the same thing as computeCost, and gives an example of coding a metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.567945064408294"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sse of a point to the center of the cluster it's assigned to\n",
    "def getsse(point):\n",
    "    this_center = my_kmmodel.centers[my_kmmodel.predict(point)]\n",
    "           #for this point get it's clustercenter\n",
    "    return (sum([x**2 for x in (point - this_center)])) \n",
    "\n",
    "\n",
    "my_sse=my_data.map(getsse).collect()  #collect list of sse of each pt to its center\n",
    "\n",
    "print \"Sum of Squared Error:\"\n",
    "np.array(my_sse).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the number of cluster, `k=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6550659149901916"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "# To generate random data RDD we need need `RandomRDD`\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "# Generate random class data and add in a cluster center to random 2D points\n",
    "c1_v = RandomRDDs.normalVectorRDD(sc,20,2,numPartitions=2,seed=1L).map(lambda v:np.add([1,5],v))\n",
    "c2_v = RandomRDDs.normalVectorRDD(sc,16,2,numPartitions=2,seed=2L).map(lambda v:np.add([5,1],v))\n",
    "c3_v = RandomRDDs.normalVectorRDD(sc,12,2,numPartitions=2,seed=3L).map(lambda v:np.add([4,6],v))\n",
    "\n",
    "# Concatenate 2 RDDs with the `.union(other)` function\n",
    "tmp_c = c1_v.union(c2_v)\n",
    "my_data = tmp_c.union(c3_v)   #this now has all points, as RDD\n",
    "\n",
    "# Train the model using k-means++\n",
    "my_kmmodel = KMeans.train(my_data,k=4,\n",
    "               maxIterations=20,runs=1,\n",
    "               initializationMode='k-means||',seed=10L)\n",
    "\n",
    "# Get the sse of a point to the center of the cluster it's assigned to\n",
    "def getsse(point):\n",
    "    this_center = my_kmmodel.centers[my_kmmodel.predict(point)]\n",
    "           #for this point get it's clustercenter\n",
    "    return (sum([x**2 for x in (point - this_center)])) \n",
    "\n",
    "\n",
    "my_sse=my_data.map(getsse).collect()  #collect list of sse of each pt to its center\n",
    "\n",
    "print \"Sum of Squared Error:\"\n",
    "np.array(my_sse).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the number of cluster, `k=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0255546450123463"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "# To generate random data RDD we need need `RandomRDD`\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "# Generate random class data and add in a cluster center to random 2D points\n",
    "c1_v = RandomRDDs.normalVectorRDD(sc,20,2,numPartitions=2,seed=1L).map(lambda v:np.add([1,5],v))\n",
    "c2_v = RandomRDDs.normalVectorRDD(sc,16,2,numPartitions=2,seed=2L).map(lambda v:np.add([5,1],v))\n",
    "c3_v = RandomRDDs.normalVectorRDD(sc,12,2,numPartitions=2,seed=3L).map(lambda v:np.add([4,6],v))\n",
    "\n",
    "# Concatenate 2 RDDs with the `.union(other)` function\n",
    "tmp_c = c1_v.union(c2_v)\n",
    "my_data = tmp_c.union(c3_v)   #this now has all points, as RDD\n",
    "\n",
    "# Train the model using k-means++\n",
    "my_kmmodel = KMeans.train(my_data,k=3,\n",
    "               maxIterations=20,runs=1,\n",
    "               initializationMode='k-means||',seed=10L)\n",
    "\n",
    "# Get the sse of a point to the center of the cluster it's assigned to\n",
    "def getsse(point):\n",
    "    this_center = my_kmmodel.centers[my_kmmodel.predict(point)]\n",
    "           #for this point get it's clustercenter\n",
    "    return (sum([x**2 for x in (point - this_center)])) \n",
    "\n",
    "\n",
    "my_sse=my_data.map(getsse).collect()  #collect list of sse of each pt to its center\n",
    "\n",
    "print \"Sum of Squared Error:\"\n",
    "np.array(my_sse).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
