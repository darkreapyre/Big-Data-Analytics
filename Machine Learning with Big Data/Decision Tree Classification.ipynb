{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification Models  \n",
    "## Introduction  \n",
    "The following Python code demonstrates some basic __Decision Tree__ classification using Spark. We will create basic Weather data for predicting if someone will 'Play' tennis. The data is hard coded as a list of lists and put into a dataframe. The dataframe is then mapped into an RDD of labeled point vectors. __Notice__ that since `numpy` cannot handle categorical variables, these are recoded as binary indicator variables. For Example, __outlook__ = `sunny`, `overcast` or `rainy`, is replaced by three variables; `sunny` (1 or 0), `overcast` (1 or 0), `rainy` (1 or 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create the `rawdata`, loosely based on the UHCI weather data set\n",
    "rawdata = [\n",
    "['sunny',85,85,'FALSE',0],\n",
    "['sunny',80,90,'TRUE',0],\n",
    "['overcast',83,86,'FALSE',1],\n",
    "['rainy',70,96,'FALSE',1],\n",
    "['rainy',68,80,'FALSE',1],\n",
    "['rainy',65,70,'TRUE',0],\n",
    "['overcast',64,65,'TRUE',1],\n",
    "['sunny',72,95,'FALSE',0],\n",
    "['sunny',69,70,'FALSE',1],\n",
    "['rainy',75,80,'FALSE',1],\n",
    "['sunny',75,70,'TRUE',1],\n",
    "['overcast',72,90,'TRUE',1],\n",
    "['overcast',81,75,'FALSE',1],\n",
    "['rainy',71,91,'TRUE',0]\n",
    "]\n",
    "\n",
    "# Create a Data Frame from the `rawdata`\n",
    "from pyspark.sql import SQLContext,Row\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data_df = sqlContext.createDataFrame(rawdata,\n",
    "   ['outlook','temp','humid','windy','play'])\n",
    "\n",
    "# Transform categorical variables into indicator variables\n",
    "out2index = {'sunny':[1,0,0],'overcast':[0,1,0],'rainy':[0,0,1]}\n",
    "\n",
    "# Make an RDD of labeled vectors\n",
    "def newrow(dfrow):\n",
    "    outrow = list(out2index.get((dfrow[0])))  #get dictionary entry for outlook\n",
    "    outrow.append(dfrow[1])   #temp\n",
    "    outrow.append(dfrow[2])   #humidity\n",
    "    if dfrow[3]=='TRUE':      #windy\n",
    "        outrow.append(1)\n",
    "    else:\n",
    "        outrow.append(0)\n",
    "    return (LabeledPoint(dfrow[4],outrow))\n",
    "\n",
    "datax_rdd=data_df.map(newrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Verify the __RDD__ data and some basic summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.0,0.0,0.0,85.0,85.0,0.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,0.0,80.0,90.0,1.0]),\n",
       " LabeledPoint(1.0, [0.0,1.0,0.0,83.0,86.0,0.0]),\n",
       " LabeledPoint(1.0, [0.0,0.0,1.0,70.0,96.0,0.0]),\n",
       " LabeledPoint(1.0, [0.0,0.0,1.0,68.0,80.0,0.0])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "dt_model = DecisionTree.trainClassifier(datax_rdd,2,{},impurity='entropy',\n",
    "          maxDepth=3,maxBins=32, minInstancesPerNode=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision tree function returns a decision tree object. We can review out the object by entering the object name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_java_loader_class',\n",
       " '_java_model',\n",
       " '_load_java',\n",
       " '_sc',\n",
       " 'call',\n",
       " 'depth',\n",
       " 'load',\n",
       " 'numNodes',\n",
       " 'predict',\n",
       " 'save',\n",
       " 'toDebugString']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxDepth and maxBins\n",
    "#{} could be categorical feature list,\n",
    "# To do regression, have no numclasses,and use trainRegression function\n",
    "dt_model\n",
    "dir(dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the Decision Tree with `toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 9 nodes\n",
      "  If (feature 1 <= 0.0)\n",
      "   If (feature 4 <= 80.0)\n",
      "    If (feature 3 <= 68.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 68.0)\n",
      "     Predict: 1.0\n",
      "   Else (feature 4 > 80.0)\n",
      "    If (feature 0 <= 0.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 1 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print dt_model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Notice__ the number of nodes are the predict (leaf nodes) and the ifs. Now we get some of training data and test the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: Confidence Matrix and Percent Correct\n",
      "[[ 5.  0.]\n",
      " [ 2.  7.]]\n",
      "0.857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:7: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "datax_col = datax_rdd.collect()   #if datax_rdd was big, use sample or take\n",
    "\n",
    "#redo the confidence  matrix code (it would be more efficient to pass a model)\n",
    "dt_cf_mat=np.zeros([2,2])  #num of classes\n",
    "for pnt in datax_col:\n",
    "    predctn = dt_model.predict(np.array(pnt.features))\n",
    "    dt_cf_mat[pnt.label][predctn]+=1\n",
    "corrcnt=0\n",
    "for i in range(2): \n",
    "    corrcnt+=dt_cf_mat[i][i]\n",
    "dt_per_corr=corrcnt/dt_cf_mat.sum()\n",
    "print 'Decision Tree: Confidence Matrix and Percent Correct'\n",
    "print dt_cf_mat\n",
    "print dt_per_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first line returned should tell us the number of nodes and the depth of the tree. Observe how the number of nodes is related to the decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Performance by introducing \"Dummy\" Variables  \n",
    "In this scenario, we re-run the model against the same data set, but introduce \"useless\" variables that is constant, after the 'play' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "#outlook,temperature,humidity,windy,play, copied from Weka's data example\n",
    "rawdata=[\n",
    "['sunny',85,85,'FALSE',0,1],\n",
    "['sunny',80,90,'TRUE',0,1],\n",
    "['overcast',83,86,'FALSE',1,1],\n",
    "['rainy',70,96,'FALSE',1,1],\n",
    "['rainy',68,80,'FALSE',1,1],\n",
    "['rainy',65,70,'TRUE',0,1],\n",
    "['overcast',64,65,'TRUE',1,1],\n",
    "['sunny',72,95,'FALSE',0,1],\n",
    "['sunny',69,70,'FALSE',1,1],\n",
    "['rainy',75,80,'FALSE',1,1],\n",
    "['sunny',75,70,'TRUE',1,1],\n",
    "['overcast',72,90,'TRUE',1,1],\n",
    "['overcast',81,75,'FALSE',1,1],\n",
    "['rainy',71,91,'TRUE',0,1]\n",
    "]\n",
    "\n",
    "from pyspark.sql import SQLContext,Row\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data_df=sqlContext.createDataFrame(rawdata,\n",
    "                                   ['outlook','temp','humid','windy','play','mydummy']) #<--add field\n",
    "\n",
    "#transform categoricals into indicator variables\n",
    "out2index={'sunny':[1,0,0],'overcast':[0,1,0],'rainy':[0,0,1]}\n",
    "\n",
    "#make RDD of labeled vectors\n",
    "def newrow(dfrow):    \n",
    "    outrow = list(out2index.get((dfrow[0])))  #get dictionary entry\n",
    "    outrow.append(dfrow[1])   #temp    \n",
    "    outrow.append(dfrow[2])   #humidity    \n",
    "    if dfrow[3]=='TRUE':      #windy        \n",
    "        outrow.append(1)    \n",
    "    else:        \n",
    "        outrow.append(0)    \n",
    "    outrow.append(dfrow[5])  # <---- add this     \n",
    "    return (LabeledPoint(dfrow[4],outrow))\n",
    "\n",
    "datax_rdd=data_df.map(newrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.0,0.0,0.0,85.0,85.0,0.0,1.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,0.0,80.0,90.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, [0.0,1.0,0.0,83.0,86.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, [0.0,0.0,1.0,70.0,96.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, [0.0,0.0,1.0,68.0,80.0,0.0,1.0])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 9 nodes\n",
      "  If (feature 1 <= 0.0)\n",
      "   If (feature 4 <= 80.0)\n",
      "    If (feature 3 <= 68.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 68.0)\n",
      "     Predict: 1.0\n",
      "   Else (feature 4 > 80.0)\n",
      "    If (feature 0 <= 0.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 1 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n",
      "Decision Tree: Confidence Matrix and Percent Correct\n",
      "[[ 5.  0.]\n",
      " [ 2.  7.]]\n",
      "0.857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "dt_model = DecisionTree.trainClassifier(datax_rdd,2,{},impurity='entropy',\n",
    "          maxDepth=3,maxBins=32, minInstancesPerNode=2)  \n",
    "\n",
    "print dt_model.toDebugString()\n",
    "\n",
    "datax_col=datax_rdd.collect()   #if datax_rdd was big, use sample or take\n",
    "\n",
    "#redo the conf. matrix code (it would be more efficient to pass a model)\n",
    "dt_cf_mat=np.zeros([2,2])  #num of classes\n",
    "for pnt in datax_col:\n",
    "    predctn = dt_model.predict(np.array(pnt.features))\n",
    "    dt_cf_mat[pnt.label][predctn]+=1\n",
    "corrcnt=0\n",
    "for i in range(2): \n",
    "    corrcnt+=dt_cf_mat[i][i]\n",
    "dt_per_corr=corrcnt/dt_cf_mat.sum()\n",
    "print 'Decision Tree: Confidence Matrix and Percent Correct'\n",
    "print dt_cf_mat\n",
    "print dt_per_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Performance on New Unseen Data    \n",
    "In this scenario, we re-run the model against the new data to measure it's performance.\n",
    "The `newpoint` corresponds to the following:\n",
    "\n",
    "- Outlook binary indicator variables = sunny,overcast,rainy\n",
    "- Temperature = 68\n",
    "- Humidy = 79\n",
    "- Windy = 0\n",
    "- Useless-constant (dummy variable) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Add a new data point\n",
    "newpoint = np.array([1,0,0,68,79,0,1])\n",
    "\n",
    "# Execute the new test point through the Naive Bayes Model\n",
    "test = dt_model.predict(newpoint)\n",
    "print test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree result is completely different from the same data executed using the __Naiive Bayes__ model. To better analyze why this is the case, we need to further analyse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 9 nodes\n",
      "  If (feature 1 <= 0.0)\n",
      "   If (feature 4 <= 80.0)\n",
      "    If (feature 3 <= 68.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 68.0)\n",
      "     Predict: 1.0\n",
      "   Else (feature 4 > 80.0)\n",
      "    If (feature 0 <= 0.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 1 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print dt_model.toDebugString()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
