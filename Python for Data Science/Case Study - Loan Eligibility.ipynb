{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The following is a case study based on an example from the *Analytics Vidhya DataHack platform* and involves a fictional company, __Dream Housing Finance__ company. This company deals in all home loans. They have a presence across all urban, semi-urban and rural areas. Customers first apply for a home loan and after that, Dream Housing Finance company validates the customer's eligibility. The company wants to automate the loan eligibility process in real-time, based on customer detail provided while filling online application form.\n",
    "\n",
    "# Downloading and Exploring the Data\n",
    "To start, we load the training and testing set into your python environment. We will use the [training](https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/train.csv) set to build our model, and the [test](https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/test.csv) set to validate it. Both the files are stored on the web as CSV files. We will load this data with the `pandas.read_csv()` function. It converts the data set to a python dataframe. In simple words, Python dataframe can be imagined as an equivalent of a spreadsheet or a SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necesary libraries and remove warnings\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import training data as train\n",
    "train = pd.read_csv(\"https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/train.csv\")\n",
    "\n",
    "# Import testing data as test\n",
    "test = pd.read_csv(\"https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/test.csv\")\n",
    "\n",
    "# Print top 5 observation of training dataset\n",
    "print \"Top 5 observations in the training dataset:\", \"\\n\", \"\\n\", train.head()\n",
    "\n",
    "# Print the dimensions of the training dataset\n",
    "print \"Number of observations in the training dataset:\", len(train)\n",
    "print \"Number of variables in the training dataset:\", len(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "Additionally we can do some summary statistics, usig the `dataframe.describe()` function, on the __numerical__ values of the training dataset. This will show us the *count*, *mean*, *standard deviation (sd)*, *min*, *quartiles* and max in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the summary statistics of the numerical variables\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the __non-numeric__ (catagorical) values like `Property_Area`, `Credit_History` etc., we can look at frequency distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequency distribution for `Property_Area`\n",
    "train['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequency distribution for `Credit_History`\n",
    "train['Credit_History'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Value Distribution\n",
    "Now that we are familiar with basic data characteristics, we next look at the distribution of numerical variables. We start by plotting the histogram of `LoanAmount` and `ApplicantIncome` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for variable LoanAmount\n",
    "train['ApplicantIncome'].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for variable LoanAmount\n",
    "train['LoanAmount'].hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further explore the `LoanAmount` distribution by breaking it down further based on the `Gender` catagory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a box plot for variable LoanAmount by variable Gender of training data set\n",
    "train.boxplot(column='LoanAmount', by = 'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catagorical Variable Distribution\n",
    "We have looked at an example of the __numerical__ distributions of `ApplicantIncome` and `LoanIncome` and an example of how an applicants *Gender* may impact the decision. The above plot highlights that the distribution between Males and Females and the Loan Amount is not exactly alligned. This warrants further exploration, for instance, we can try see whether `Gender` is affecting the loan status or not. This can be tested using cross-tabulation, using the `crosstab()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a crosstab of `Gender` and `Loan_Status`\n",
    "pd.crosstab(train['Gender'], train['Loan_Status'], margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it would seem that there are more loans approved for Males as opposed to Females, $339$ vs. $75$. But if we look closer, there are also more Male applicants than Female applications. So let's try and understand this within the context of proportions, using the `apply()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to convert to percentages\n",
    "def percentages(x):\n",
    "  return x/float(x[-1])\n",
    "\n",
    "# Create a crosstab of `Gender~ and `Loan_status` and `apply` the above function \n",
    "pd.crosstab(train [\"Gender\"], train [\"Loan_Status\"], margins = True).apply(percentages, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the proportions, we can see there is in fact no Gender discrimination as the amount of loan application approvals (as well as loan aplication rejections) between Males and Females are fairly equal. \n",
    "\n",
    "So from a data exploration exercise, it seems that we have an even distribution of data (both numerical and catagorical) in order to proceed.\n",
    "\n",
    "# Cleaning the Data\n",
    "### Dealing with Missing Values\n",
    "Rarely is the data captured perfectly, and especially in the process of applying for a Home Loan, people may not disclose all details or the data may not be entered correctly. So the next task is look of potential missing values using the `isnull()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look for missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though there are missing values in the dataset, not all of them impact wether or not a loan application is approved, however variables like `LoanAmount` are import. So the next step is to impute the missing values in the important variables.\n",
    "\n",
    "There are multiple ways to fill the missing values of continuous variables. You can replace them with mean, median or estimate values based on other features of the data set. For the sake of simplicity, we would impute the missing values of `LoanAmount` by the __mean__ of the values for `LoanAmount` using the `fillna()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of the values for `LoanAmount`\n",
    "print \"Mean of the values for LoanAmount:\", round(train['LoanAmount'].mean())\n",
    "\n",
    "# Impute the missing values with the mean\n",
    "train['LoanAmount'].fillna(train['LoanAmount'].mean(), inplace = True)\n",
    "\n",
    "# Confirm no missing values\n",
    "print \"\\n\", \"Does LoanAmount have missing values?\", \"\\n\", train['LoanAmount'].isnull().sum() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about catagorical variables, like `Self_Employed`? To impute missing values of Categorical variables, we look at the frequency table. The simplest way is to impute with value which has highest frequency because there is a higher probability of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the distribution of variabes\n",
    "print train['Self_Employed'].value_counts()\n",
    "\n",
    "# Print the percentage of 'No'\n",
    "print \"\\n\", \"Precentage of No's:\", \\\n",
    "round(train['Self_Employed'].value_counts()['No']/train['Self_Employed'].value_counts().sum() * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the distribution of `SelfEmployed`, $500$ out of $582$ which is aproximately $86%$ of total values falls under the category \"No\". So to impute the missing values in `SelfEmployed`, we will replace missing values with __No__. We will also apply this methadology to other important catagorical variables, `Gender` and `Credit_History`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute missing catagorical values\n",
    "train['Self_Employed'].fillna('No', inplace = True)\n",
    "train['Gender'].fillna('Male', inplace = True)\n",
    "train['Credit_History'].fillna(1, inplace = True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Outliers\n",
    "Now that we have dealt with missing values, we can recall when plotting the distributions, that there are some extreme values. These values are important, so we don't want to discard them. For example, if we look at the `LoanAmount` variable we have some of these extreme values, which are completely viable as some applicants may require larger amounts depending on specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of `LoanAmount`\n",
    "train['LoanAmount'].hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's important to note though that when making predictions, these extreme values can potentially skew the results. So instead of treating them as outliers, we can try a log transformation to nullify their effect. The __numpy__ library has a number of functions like `log()` that allow us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new varaible of the log transformation on `LoanAmount`\n",
    "train['LoanAmount_log'] = np.log(train['LoanAmount'])\n",
    "\n",
    "# Plot the new variable\n",
    "train['LoanAmount_log'].hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.\n",
    "\n",
    "Next we can also consolidate some of the data to make our predictions easier. For instance, it is somewhat pointless to factor both the applicant's income as well as co-applicant's income as separate since in most cases, loan approval is based on joint income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new variable `TotalIncome`\n",
    "train['TotalIncome'] = train['ApplicantIncome'] + train['CoapplicantIncome']\n",
    "\n",
    "# Plot the new variable\n",
    "train['TotalIncome'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can been seen, this new variable also needs to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new normalized variable\n",
    "train['Log_TotalIncome'] = np.log(train['TotalIncome'])\n",
    "\n",
    "# Plot this new variable\n",
    "train['Log_TotalIncome'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "Since the scope of this case study is to make predictions of loan eligibility, we employ the __Scikit Learn__ library to start this process. It is important to note that Scikit Learn only works with numeric array. Hence, we need to label all the character variables into a numeric array. For example, the variable `Gender` has two labels, Male and Female. Hence, we will transform the labels to number as __1__ for *Male* and __0__ for *Female*. To do this, we will employ the module called `LabelEncoder`. This module helps to label character labels into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert Male to 1 and Female to 0 in `Gender`\n",
    "number = LabelEncoder()\n",
    "train['Gender'] = number.fit_transform(train['Gender'].astype(str))\n",
    "\n",
    "# Apply the same process to `Married`\n",
    "train['Married'] = number.fit_transform(train['Married'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle behind selecting the right algorithm is to look at the dependent variable (or target variable). In this case study, it's __Loan Prediction__. Thus, we need to classify a customer's eligibility for Loan as *Y* or *N* based on the available information about the customer. Here the dependent variable is categorical and our task is to classify the customer in two groups; eligible for the loan amount and not eligible for the loan amount. This is a classification exercise, so we will initialy just try __Logistic Regression__. \n",
    "\n",
    "Before starting the classification exercise, we will prepare the environment by doing the following:\n",
    "1. Import the necessary libaraies.\n",
    "2. Download a fresh set of training and test datasets.\n",
    "3. Pre-process the data. (i.e. fill in missng vales; encode catacogical variables; add `TotalIncome` etc.)\n",
    "4. Create a generic function for the classification model.\n",
    "\n",
    "Using these steps, will allow us to create a framework that can be re-used to execute other models without having to manually perform transformations or manually generate customizations for each algorithm and as such, framework can be applied later to other __Classification__ alorithms.\n",
    "\n",
    "__Note:__ The `classify()` function also includes a cross-validation step to help improve model accuracy, before applying the model to test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Re-import Import libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold   # For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier # For later testing\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz # For later testing\n",
    "from sklearn import metrics\n",
    "\n",
    "# Download a fresh training and testing data\n",
    "train_modified = pd.read_csv(\"https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/train.csv\")\n",
    "test_modified = pd.read_csv(\"https://s3-ap-southeast-1.amazonaws.com/av-datahack-datacamp/test.csv\")\n",
    "\n",
    "# Clean the training data\n",
    "train_modified['Gender'].fillna(train_modified['Gender'].mode()[0], inplace = True)\n",
    "train_modified['Married'].fillna(train_modified['Married'].mode()[0], inplace = True)\n",
    "train_modified['Dependents'].fillna(train_modified['Dependents'].mode()[0], inplace = True)\n",
    "train_modified['Credit_History'].fillna(train_modified['Credit_History'].mode()[0], inplace = True)\n",
    "catagoricals = ['Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area']\n",
    "for var in catagoricals:\n",
    "    label = LabelEncoder()\n",
    "    train_modified[var]=label.fit_transform(train_modified[var].astype('str'))\n",
    "train_modified['LoanAmount'].fillna(train_modified['LoanAmount'].mean(), inplace = True)\n",
    "train_modified['TotalIncome'] = train_modified['ApplicantIncome'] + train_modified['CoapplicantIncome']\n",
    "train_modified['Log_TotalIncome'] = np.log(train_modified['TotalIncome'])\n",
    "\n",
    "# Clean the testing data\n",
    "test_modified['Gender'].fillna(test_modified['Gender'].mode()[0], inplace = True)\n",
    "test_modified['Married'].fillna(test_modified['Married'].mode()[0], inplace = True)\n",
    "test_modified['Dependents'].fillna(test_modified['Dependents'].mode()[0], inplace = True)\n",
    "test_modified['Credit_History'].fillna(test_modified['Credit_History'].mode()[0], inplace = True)\n",
    "for var in catagoricals:\n",
    "    label = LabelEncoder()\n",
    "    test_modified[var]=label.fit_transform(test_modified[var].astype('str'))\n",
    "test_modified['LoanAmount'].fillna(test_modified['LoanAmount'].mean(), inplace = True)\n",
    "test_modified['TotalIncome'] = test_modified['ApplicantIncome'] + test_modified['CoapplicantIncome']\n",
    "test_modified['Log_TotalIncome'] = np.log(test_modified['TotalIncome'])\n",
    "\n",
    "# Create a generatic classification function that takes the data and model as inputs\n",
    "def classify(model, data, predictors, outcome):\n",
    "    # Model fit\n",
    "    model.fit(data[predictors], data[outcome])\n",
    "    \n",
    "    # Predict on training data\n",
    "    predict = model.predict(data[predictors])\n",
    "    \n",
    "    # Score the result\n",
    "    score = metrics.accuracy_score(predict, data[outcome])\n",
    "    print \"Accuracy Score: %s\" % \"{0:.3%}\".format(score)\n",
    "    \n",
    "    # Perform K-fold cross-validation with generic 10 folds\n",
    "    cv = KFold(data.shape[0], n_folds = 10)\n",
    "    error = [] #List of error results\n",
    "    for train, test in cv:\n",
    "        # Separate training and cross-validation test set\n",
    "        train_predictors = (data[predictors].iloc[train,:]) # Index train data\n",
    "        target_set = data[outcome].iloc[train] # Index the target data\n",
    "        \n",
    "        # Train the model based on predictors on the target indexes\n",
    "        model.fit(train_predictors, target_set)\n",
    "        \n",
    "        # Record the core for each fold run on cross-validation set\n",
    "        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    "        \n",
    "    # Print the mean error results list\n",
    "    print \"Cross-validation error: %s\" % \"{0:.3%}\".format(np.mean(error))\n",
    "    \n",
    "    # Execute the model fit again outside the function\n",
    "    model.fit(data[predictors], data[outcome])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression is a classification algorithm that is used to predict a binary outcome given a set of independent variables. Basically, it predicts the probability of occurrence of an event by fitting data to a logit function. The `LogisticRegression()` function is part of `linear_model` module of sklearn and is used to create logistic regression. One way to start would be to take all the variables into the model, but this might result in overfitting, for example, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. So based on some of the intuition gleemed from our exploratory analysis, we can easily make some hypothesis to start with.\n",
    "\n",
    "The chances of getting a loan will be higher for:\n",
    "- Applicants having a credit history.\n",
    "- Applicants with higher applicant and co-applicant income.\n",
    "- Applicants with higher education level.\n",
    "- Properties in urban areas with high growth perspectives.\n",
    "- Applicants employment status.\n",
    "\n",
    "Using the above criteria as predictors, we will build the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select based on criteria for getting a loan\n",
    "predictors = ['Credit_History', 'Education', 'Self_Employed', 'TotalIncome', 'Property_Area']\n",
    "\n",
    "# Converting predictors and outcome to numpy array\n",
    "x_train = train_modified[predictors].values\n",
    "y_train = train_modified['Loan_Status'].values\n",
    "\n",
    "# Model Building\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The objective of this case study is to automate the loan eligibility process in real-time, based on customer detail provided while filling online application form. So now that we have out model, we can use it with the important predictors and apply it to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert predictors and outcome to numpy array\n",
    "x_test = test_modified[predictors].values\n",
    "\n",
    "# Predict Output\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "# Add the outcome to a final dataset\n",
    "lr_final = test_modified\n",
    "lr_final['Loan_Status'] = predicted\n",
    "\n",
    "# Output test dataset sample\n",
    "lr_final[['Loan_ID', 'Loan_Status']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though the objective of the case study has been met, we still don't know the accuracy of the model's predictions and if the model can be improved at all. To this end, we will employ the `classify()` functioned. This function will provide the accuracy of the model as well a execute a 10 fold cross-validation to see if the accuracy can be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome = 'Loan_Status'\n",
    "model = LogisticRegression()\n",
    "predictors = ['Credit_History', 'Education', 'Self_Employed', 'TotalIncome', 'Property_Area']\n",
    "classify(model, train_modified, predictors, outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So our initial model only has a 80% accuracy and even employing a cross-validation process to try and improve the accurancy doesn't help. To address these issues, we will further investigate applying better modeling techniques and different models like __Decision Trees__.\n",
    "\n",
    "### Decision Trees\n",
    "Decision Trees are mostly used in classification problems as they work for both categorical and continuous input and output variables. In this technique, we basically split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter/differentiator in input variables. This model is known to provide a higher degree of accuracy over Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manual implementation of a Decision Tree\n",
    "import sklearn.tree\n",
    "\n",
    "# Select three predictors Credit_History, Education and Gender\n",
    "predictors =['Credit_History', 'Education', 'Self_Employed', 'TotalIncome', 'Property_Area']\n",
    "\n",
    "# Converting predictors and outcome to numpy array\n",
    "x_train = train_modified[predictors].values\n",
    "y_train = train_modified['Loan_Status'].values\n",
    "\n",
    "# Model Building\n",
    "model = sklearn.tree.DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Converting predictors and outcome to numpy array\n",
    "x_test = test_modified[predictors].values\n",
    "\n",
    "#Predict Output\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "# Add the outcome to a final dataset\n",
    "dt_final = test_modified\n",
    "dt_final['Loan_Status'] = predicted\n",
    "\n",
    "# Output test dataset sample\n",
    "dt_final[['Loan_ID', 'Loan_Status']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Can already see differnt prediciton, but how accurate was the model?__ therefore try `classify()` function with `DecisionTreeClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome = 'Loan_Status'\n",
    "model = DecisionTreeClassifier()\n",
    "predictors = ['Credit_History', 'Education', 'Self_Employed', 'TotalIncome', 'Property_Area']\n",
    "classify(model, train_modified, predictors, outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__blah blah blah__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
