{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK  IN PROGRESS\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis using Spark and Python  \n",
    "Now that we have an idea of how to explore some data in Spark, the following content describes how to apply some of those principles to the __Exploratory Data Analysis__ methodology within Data Science. This document outlines some of the pitfalls and issues that one may encounter as they they try to explore data in Spark.\n",
    "\n",
    "__Note:__ The infomration within this document is based on the [Python Tutorials](https://www.codementor.io/python/tutorial) from __Code Mentor__. \n",
    "\n",
    "\n",
    "## Getting the Data  \n",
    "### Getting the Data  \n",
    "For this exercise, we will use the Incidents derived from [SFPD Crime Incident Reporting system](https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry\n",
    ").  \n",
    "\n",
    "The Data isfomatted to show the following infortmation:\n",
    "- Incident Number\n",
    "- Catagory of the Incident\n",
    "- Day of the Week\n",
    "- Date\n",
    "- Time\n",
    "- Police Department District\n",
    "- Resolution\n",
    "- Address\n",
    "- X map coordinates\n",
    "- Y map coordinates\n",
    "- Map location\n",
    "- Poilice Deprtment ID\n",
    "\n",
    "The data has been exported to `.csv` format and copied to HDFS using the following example proceedure:\n",
    "```\n",
    "wget https://data..org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD -O SFPD_Incidents.csv\n",
    "hdfs dfs -put incidents.csv /data/\n",
    "hdfs dfs -ls /data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data into Spark  \n",
    "#### Manual Schema Preparation  \n",
    "The first step to doing this is to isolate the headers of the data to be used for the field names,in order to understand what the fields are for and hence the field types. Since this is a manual process, we will manually bring the data back into Spark using the `SQLContext`.  We will not be using any of the functionality of dataframes and `spark-csv` adn the reson for this is to highlight the ease of doing this with dataframes later, over the manual (or traditional) steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "# Local fiile if using Spark.local\n",
    "#data = sc.textFile(\"file:///vagrant/notebook.tmp/data/SFPD_Incidents.csv\")\n",
    "# If using HDFS on Spark.vsphere\n",
    "data = sc.textFile(\"hdfs://master:54310/data/SFPD_Incidents.csv\")\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is, as the [documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema) suggestions is to isolate the headings. We wil use these headings to build the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = data.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can construct the individual fields, by splitting them up based on the `,` delimeter. As a baseline, we force each field to be of type `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in header.split(',')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have individual fields, we can specify the exact type of data within each column based on the description from the origional source. For example, according to the website, the `DayOfWeek` column is of __Plain Text__ type, but the `Date` column is is of type, __Date & Time__. So all we need to do is change the type of data in each of our fields, to match the descript from the website.  \n",
    "\n",
    "Therfore, the fonly fields we need to change are:\n",
    "- __Date__ from `StringType` to `DateType` or `LongType`\n",
    "- __Time__ from `StringType` to `TimestampType` or leave it as a `string`\n",
    "- __X__ from `StringType` to `FloatType` or `DoubleType`\n",
    "- __Y__ from `StringType` to `FloatType` or `DoubleType`\n",
    "- __PdId__ from `StringType` to `LongType`\n",
    "- Potentilly change __IncdntNum__ to `LongType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the necessary fields to the proper type based on my assumptions\n",
    "#fields[4].dataType = DateType() #Date\n",
    "#fields[5].dataType = TimestampType() #Time\n",
    "#fields[9].dataType = FloatType() #X\n",
    "#fields[10].dataType = FloatType() #Y\n",
    "#fields[12].dataType = LongType() #PdId\n",
    "\n",
    "# Set the fields based on what pandas' picked up\n",
    "fields[0].dataType = LongType()\n",
    "fields[9].dataType = DoubleType()\n",
    "fields[10].dataType = DoubleType()\n",
    "fields[12].dataType = LongType()\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the data aquisition process, extracing the headers, also provides an opportunity to clean them up. Although this is not necessary, we can change the headings to something that's more understandable. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change `IncidntNum` to `Incident`\n",
    "fields[0].name = \"Incident\"\n",
    "\n",
    "# Change `DayOfWeek` to `Day`\n",
    "fields[3].name = \"Day\"\n",
    "\n",
    "# Change `Descript` to `Description`\n",
    "fields[2].name = \"Description\"\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that the data types have been changes, we can use this to contruct the schema. This will be used later as we construct the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the schema\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the dataframe manually, a good practice is to strip out the header file so a to not conflict with the actual data using Spark's `subtract()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataHeader = data.filter(lambda x: \"PdId\" in x)\n",
    "dataHeader.collect()\n",
    "\n",
    "# Remove the header data collected\n",
    "dataNoHeader = data.subtract(dataHeader)\n",
    "dataNoHeader.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the first row starts with the actual data we can use the raw data and the schema to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(dataNoHeader, schema)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above result, one needs to have a very definite understanding on the the type of data they are dealing with and keeping in mind that we are working with __Big Data__, we will see that not all of the raw data in the rows conforms to the specifiec schema we have created. So another option to leverage `spark-csv`.\n",
    "\n",
    "#### Using Spark-csv  \n",
    "The first proceedure we will use to get the data into Spark, is `spark-csv` from [__Databricks__](http://spark-packages.org/package/databricks/spark-csv). This package allows us to import `.csv` data into a Spark DataFrame, using the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Local downladed file if using Spark.local\n",
    "#data = \"file:///vagrant/notebook.tmp/data/SFPD_Incidents.csv\"\n",
    "\n",
    "# HDFS location of the downloaded file if using Spark.vsphere\n",
    "data = \"hdfs://master:54310/data/SFPD_Incidents.csv\"\n",
    "    \n",
    "# Create a sqlContext variable to read and load the file, captuing the header and schema\n",
    "df = sqlContext.read.load(data,\n",
    "                          format=\"com.databricks.spark.csv\",\n",
    "                          header=\"true\",\n",
    "                          infereSchema=\"true\")\n",
    "\n",
    "# Take the first row\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few of important things to note from the output above. __Firstly__, the raw fomatting may not be helpful in descirbing the data. Therefore, another option to display this is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the first row\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show()` function attempts to display the formatting better, but may not be the best display output if the number of colums exceeds the width of the Notebook. __Secondly__, although `inferSchema` is set to `true`, `spark-csv` tries its best to fully capture the Schema of the data as scale, as is seen from the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the Schema\n",
    "df.printSchema()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the inferred Schema is set to string. __Thirdly__, calling the `.csv` file from the local filesystem seems to produce errors stating that the file cannot be found. I'm assuming that this is becuase the file needs to be on all nodes of the Spark Cluster and not just the Master node. To circumvent this issue, the data file should be copied copied onto HDFS - or some other shared filesystem - to ensure that all nodes can access the data.\n",
    "\n",
    "__Side Note:__ It is possible what once the Data has been captured as a Spark Dataframe, it can be comnverted to a __Pandas__ dataframe by making use of the `toPandas()` function on the Spark DataFrame, as shown below. Pandas offers a number of differences over Spark dataframes. For more information on this, see [6 differences between Pandas and Spark DataFrames](https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2#.x2a9hwn4z).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pddf = df.toPandas()\n",
    "pddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pandas dataframe are displayed in a better format, but we still need to see how Pandas enterprets the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, by converting to a Pandas dataframe, the class of the data is now converted to an `object`. So once again we still don't have a clear idea of the actual schema. So we will have to manually prepare the schema. Fortunately Spark `1.5` introduced a number of better ways to work with data types within newly created dataframes (instead of the raw `RDD`), without having to manually build and test the schema. As can be seen below we will make use of the [pyspark.sql.DataFrame](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame) class methods [withColumn](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) and [withColumnRenamed](https://spark.apache.org/docs/1.5.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed) to change the data type and even rename the colums from within the dataframe directly by referencing the actual column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Re-create the dataframe using `spark-csv`\n",
    "data = \"hdfs://master:54310/data/SFPD_Incidents.csv\"\n",
    "df = sqlContext.read.load(data,\n",
    "                          format=\"com.databricks.spark.csv\",\n",
    "                          header=\"true\",\n",
    "                          infereSchema=\"true\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure to import pyspark DataTypes\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = (df.withColumn(\"IncidntNum\", df.IncidntNum.cast(LongType()))\n",
    "#     .withColumn(\"Date\", df.Date.cast(DateType()))\n",
    "#     .withColumn(\"Time\", df.Time.cast(TimestampType()))\n",
    "     .withColumn(\"X\", df.X.cast(DoubleType()))\n",
    "     .withColumn(\"Y\", df.Y.cast(DoubleType()))\n",
    "     .withColumn(\"PdId\", df.PdId.cast(LongType()))\n",
    "     .withColumnRenamed(\"IncidntNum\", \"Incident\")\n",
    "     .withColumnRenamed(\"DayOfWeek\", \"Day\")\n",
    "     .withColumnRenamed(\"Descript\", \"Description\")\n",
    "     .withColumnRenamed(\"PdDistrict\", \"District\")\n",
    "     )\n",
    "\n",
    "# Confirm the new changes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now that it's working show `describe()` to display neat format blah blah blah*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.select([x for x in df.columns if x not in {\"X\", \"Y\", \"Location\", \"PdId\"}])\n",
    "\n",
    "\n",
    "#from functools import reduce\n",
    "#from pyspark.sql import DataFrame\n",
    "#df = reduce(DataFrame.drop, [\"X\", \"Y\", \"Location\", \"PdId\"], df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Using Pandas  and JSON\n",
    "Pandas also provides a method of reading `.csv` files, which can then be used as a Spark DataFrame. For an example on how to work with a `.csv` file in Pandas, see [Chris Albon's](http://chrisalbon.com/python/pandas_dataframe_importing_csv.html) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_csv = pd.read_csv(\"data/SFPD_Incidents.csv\")\n",
    "pd_df = sqlContext.createDataFrame(pd_csv)\n",
    "pd_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dosn't have to be on HDFS\n",
    "- (Confirm) use of broadcast variables for the above statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_df.printSchema()\n",
    "pd_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_df.show(1)\n",
    "pd_csv.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_csv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using SQLContext.read.json on a JSON file.\n",
    "\n",
    "Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(\"hdfs://master:54310/data/incidents.json\", format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_csv = \"hdfs://master:54310/data/incidents.csv\"\n",
    "df = sqlContext.read.load(input_csv, format='com.databricks.spark.csv', header='true', infereSchema='true')\n",
    "#df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"hdfs://master:54310/data/incidents.csv\")\n",
    "#df.printSchema()\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
