{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK  IN PROGRESS\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis using Spark and Python  \n",
    "Now that we have an idea of how to explore some data in Spark, the following content describes how to apply some of those principles to the __Exploratory Data Analysis__ methodology within Data Science. \n",
    "\n",
    "__Note:__ The infomration within this document is based on the [Python Tutorials](https://www.codementor.io/python/tutorial) from __Code Mentor__. \n",
    "\n",
    "\n",
    "## Getting the Data  \n",
    "### Getting the Data  \n",
    "https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget https://data.sfgov.org/api/views/tmnf-yvry/rows.json?accessType=DOWNLOAD -O ~/downloads/incidents.json\n",
    "#!wget https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD -O ~/downloads/incidents.csv\n",
    "!hdfs dfs -put ~/downloads/incidents.json /data/\n",
    "!hdfs dfs -ls /data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data in Spark (from CSV)\n",
    "#### Manual Process  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put ~/downloads/incidents.csv /data/\n",
    "!hdfs dfs -ls /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "incidentsFile = sc.textFile(\"hdfs://master:54310/data/incidents.csv\")\n",
    "incidentsFile.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah blah blah isolate header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = incidentsFile.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in header.split(',')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using spark-csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IncidntNum=u'160190088', Category=u'WARRANTS', Descript=u'ENROUTE TO OUTSIDE JURISDICTION', DayOfWeek=u'Friday', Date=u'03/04/2016', Time=u'23:46', PdDistrict=u'TENDERLOIN', Resolution=u'ARREST, BOOKED', Address=u'GOLDEN GATE AV / HYDE ST', X=u'-122.415508242782', Y=u'37.7816542806076', Location=u'(37.7816542806076, -122.415508242782)', PdId=u'16019008862050')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_csv = \"hdfs://master:54310/data/incidents.csv\"\n",
    "df = sqlContext.read.load(input_csv, format='com.databricks.spark.csv', header='true', infereSchema='true')\n",
    "#df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"hdfs://master:54310/data/incidents.csv\")\n",
    "#df.printSchema()\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IncidntNum=160190088, Category=u'WARRANTS', Descript=u'ENROUTE TO OUTSIDE JURISDICTION', DayOfWeek=u'Friday', Date=u'03/04/2016', Time=u'23:46', PdDistrict=u'TENDERLOIN', Resolution=u'ARREST, BOOKED', Address=u'GOLDEN GATE AV / HYDE ST', X=-122.415508242782, Y=37.7816542806076, Location=u'(37.7816542806076, -122.415508242782)', PdId=16019008862050)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd_csv = pd.read_csv('incidents.csv')\n",
    "pd_df = sqlContext.createDataFrame(pd_csv)\n",
    "pd_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data into Spark (from JSON)\n",
    "\n",
    "## Exploring the Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#url = 'https://www.quandl.com/api/v3/datasets/BLSE/CES9000000010.csv'\n",
    "#file = './data/SFPD_Incidents.csv'\n",
    "#f = urllib.urlretrieve(url, file)\n",
    "#df = pd.read_csv(file, index_col = 0, thousands  = ',').T\n",
    "#df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#from pyspark.sql import SQLContext\n",
    "#import pandas as pd\n",
    "\n",
    "#pandas_df = pd.read_csv('hdfs://localhost/data/data.csv')  # assuming the file contains a header\n",
    "# pandas_df = pd.read_csv('file.csv', names = ['column 1','column 2']) # if no header\n",
    "#s_df = sql_sc.createDataFrame(pandas_df)\n",
    "\n",
    "file = sc.textFile('hdfs://master:54310/data/incidents.csv')\n",
    "file.take(5)\n",
    "#file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using SQLContext.read.json on a JSON file.\n",
    "\n",
    "Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(\"hdfs://master:54310/data/incidents.json\", format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_csv = \"hdfs://master:54310/data/incidents.csv\"\n",
    "df = sqlContext.read.load(input_csv, format='com.databricks.spark.csv', header='true', infereSchema='true')\n",
    "#df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"hdfs://master:54310/data/incidents.csv\")\n",
    "#df.printSchema()\n",
    "df.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
