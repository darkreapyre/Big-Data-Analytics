{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK  IN PROGRESS\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis using Spark and Python  \n",
    "Now that we have an idea of how to explore some data in Spark, the following content describes how to apply some of those principles to the __Exploratory Data Analysis__ methodology within Data Science. \n",
    "\n",
    "__Note:__ The infomration within this document is based on the [Python Tutorials](https://www.codementor.io/python/tutorial) from __Code Mentor__. \n",
    "\n",
    "\n",
    "## Getting the Data  \n",
    "### Getting the Data  \n",
    "For this exercise, we will use the Incidents derived from [SFPD Crime Incident Reporting system](https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry\n",
    "), showing data from __1/1/2003__ up until two weeks ago from current date (__3/25/2016__).  \n",
    "\n",
    "The Data isfomatted to show the following infortmation:\n",
    "- Incident Number\n",
    "- Catagory of the Incident\n",
    "- Day of the Week\n",
    "- Date\n",
    "- Time\n",
    "- Police Department District\n",
    "- Resolution\n",
    "- Address\n",
    "- X map coordinates\n",
    "- Y map coordinates\n",
    "- Map location\n",
    "- Poilice Deprtment ID\n",
    "\n",
    "The data has been exported to `.csv` format and copied to HDFS using the following proceedure:\n",
    "\n",
    "```\n",
    "# Download and copy file to HDFS\n",
    "wget https://data..org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD -O incidents.csv\n",
    "hdfs dfs -put incidents.csv /data/\n",
    "hdfs dfs -ls /data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data into Spark  \n",
    "#### Using Spark-csv  \n",
    "The first proceedure we will use to get the data into Spark, is `spark-csv` from [__Databricks__](http://spark-packages.org/package/databricks/spark-csv). This package allows us to import `.csv` data into a Spark DataFrame, using the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HDFS location of the downloaded file\n",
    "input_csv = \"hdfs://master:54310/data/incidents.csv\"\n",
    "\n",
    "# Create a sqlContext variable to read and load the file, captuing the header and schema\n",
    "df = sqlContext.read.load(input_csv,\n",
    "                          format=\"com.databricks.spark.csv\",\n",
    "                          header=\"true\",\n",
    "                          infereSchema=\"true\")\n",
    "\n",
    "# Take the first row\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few of important things to note from the output above. __Firstly__, the raw fomatting may not be helpful in descirbing the data. Therefore, another option to display this is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the first row\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show()` function attempts to display the formatting better, but may not be the best display output if the number of colums exceeds the width of the Notebook. __Secondly__, although `inferSchema` is set to `true`, `spark-csv` was not able to fully capture the Schema of the data, as is seen from the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the Schema\n",
    "df.printSchema()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is no inferred Schema as all of the data type is set to __string__. This will need to be addressed when futher exploring the data.  \n",
    "\n",
    "__Thirdly__, calling the `.csv` file from the local filesystem seems to produce errors stating that the file cannot be found. I'm assuming that this is becuase the file needs to be on all nodes of the Spark Cluster and not just the Master node. To circumvent this issue, the data file has been copied onto HDFS - as shown at the outset - to ensure that all nodes can access the data.\n",
    "\n",
    "As a side note, it is possible what once the Data has been captured as a Spark Dataframe, it can be comnverted to a __Pandas__ dataframe by making use of the `toPandas()` function on the Spark DataFrame, as shown below. \n",
    "```\n",
    "# Example to create Pandas dataframe\n",
    "df.toPandas().head(1)\n",
    "```\n",
    "Pandas offers a number of differences over Spark dataframes. For more information on this, see [6 dofferences between Pandas and Spark DataFrames](https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2#.x2a9hwn4z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas  \n",
    "Pandas also provides a method of reading `.csv` files, which can then be used as a Spark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_csv = pd.read_csv(\"incidents.csv\")\n",
    "pd_df = sqlContext.createDataFrame(pd_csv)\n",
    "pd_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_df.show(1)\n",
    "pd_csv.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_df.printSchema()\n",
    "pd_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_csv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mostly of type = `string`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "incidentsFile = sc.textFile(\"hdfs://master:54310/data/incidents.csv\")\n",
    "incidentsFile.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah blah blah isolate header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = incidentsFile.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in header.split(',')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data into Spark (from JSON)\n",
    "\n",
    "## Exploring the Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#url = 'https://www.quandl.com/api/v3/datasets/BLSE/CES9000000010.csv'\n",
    "#file = './data/SFPD_Incidents.csv'\n",
    "#f = urllib.urlretrieve(url, file)\n",
    "#df = pd.read_csv(file, index_col = 0, thousands  = ',').T\n",
    "#df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#from pyspark.sql import SQLContext\n",
    "#import pandas as pd\n",
    "\n",
    "#pandas_df = pd.read_csv('hdfs://localhost/data/data.csv')  # assuming the file contains a header\n",
    "# pandas_df = pd.read_csv('file.csv', names = ['column 1','column 2']) # if no header\n",
    "#s_df = sql_sc.createDataFrame(pandas_df)\n",
    "\n",
    "file = sc.textFile('hdfs://master:54310/data/incidents.csv')\n",
    "file.take(5)\n",
    "#file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using SQLContext.read.json on a JSON file.\n",
    "\n",
    "Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(\"hdfs://master:54310/data/incidents.json\", format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_csv = \"hdfs://master:54310/data/incidents.csv\"\n",
    "df = sqlContext.read.load(input_csv, format='com.databricks.spark.csv', header='true', infereSchema='true')\n",
    "#df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"hdfs://master:54310/data/incidents.csv\")\n",
    "#df.printSchema()\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
